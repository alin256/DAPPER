<!-- Search file for "CHANGE" for my own changes -->
<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dapper.tools.remote.uplink API documentation</title>
<meta name="description" content="Tools related to running experimentes remotely â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<link rel="preconnect" href="https://www.google.com">
<script async src="https://cse.google.com/cse.js?cx=017837193012385208679:pey8ky8gdqw"></script>
<style>
.gsc-control-cse {padding:0 !important;margin-top:1em}
body.gsc-overflow-hidden #sidebar {overflow: visible;}
</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="https://raw.githubusercontent.com/nansencenter/DAPPER/master/docs/imgs/logo.png">
<!-- Dont work coz pdoc already defines these:
<title>DAPPER doc</title>
<meta name="description" content="Data Assimilation with Python: a Package for Experimental Research" />
-->
<a href="https://github.com/nansencenter/DAPPER" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dapper.tools.remote.uplink</code></h1>
</header>
<section id="section-intro">
<p>Tools related to running experimentes remotely</p>
<p>Requires rsync, gcloud and ssh access to the DAPPER cluster.</p>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/nansencenter/DAPPER/blob/master/dapper/tools/remote/uplink.py#L1-L373" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;Tools related to running experimentes remotely

Requires rsync, gcloud and ssh access to the DAPPER cluster.
&#34;&#34;&#34;

# TODO 9: use Fabric? https://www.fabfile.org/

import os
import subprocess
import tempfile
import time
from datetime import timedelta, timezone

from dateutil.parser import parse as datetime_parse
from patlib.std import sorted_human
from tqdm import tqdm

from dapper.dpr_config import rc


class SubmissionConnection:
    &#34;&#34;&#34;Establish multiplexed ssh to a given submit-node for a given xps_path.&#34;&#34;&#34;

    def __init__(self,
                 xps_path,
                 name=&#34;condor-submit&#34;,
                 zone=&#34;us-central1-f&#34;,
                 proj=&#34;mc-tut&#34;):
        # Job info
        self.xps_path = xps_path
        self.nJobs    = len(list_job_dirs(xps_path))
        # Submit-node info
        self.name     = name
        self.proj     = proj
        self.zone     = zone
        self.host     = f&#34;{name}.{zone}.{proj}&#34;
        # instance name (as viewed by system ssh)
        self.ip       = get_ip(name)

        print(&#34;Preparing ssh connection&#34;)
        sub_run(&#34;gcloud compute config-ssh&#34;, shell=True)
        # Use multiplexing to enable simultaneous connections.
        # Possible alternative: alter MaxStartups in sshd_config,
        # or other configurations:
        # - https://stackoverflow.com/a/36654900/38281
        # - https://unix.stackexchange.com/a/226460
        # - https://superuser.com/a/1032667/142925
        self.ssh_M = (
            &#39;&#39;&#39;ssh -o ControlMaster=auto&#39;&#39;&#39;
            &#39;&#39;&#39; -o ControlPath=~/.ssh/%r@%h:%p.socket -o ControlPersist=1m&#39;&#39;&#39;)

        # print_condor_status()
        print(&#34;autoscaler.py%s detected&#34; % (&#34;&#34; if _detect_autoscaler(self) else &#34; NOT&#34;))

    def remote_cmd(self, cmd_string, **kwargs):
        &#34;&#34;&#34;Run command at self.host via multiplexed ssh.&#34;&#34;&#34;
        # Old version (uses gcloud):
        #     command = &#34;&#34;&#34;--command=&#34;&#34;&#34; + command
        #     connect = &#34;gcloud compute ssh condor-submit&#34;.split()
        #     output = sub_run(connect + [command])
        return sub_run([*self.ssh_M.split(), self.ip, cmd_string], **kwargs)

    def rsync(self, src, dst, opts=(), rev=False, prog=False, dry=False, use_M=True):
        # Prepare: opts
        if isinstance(opts, str):
            opts = opts.split()

        # Prepare: src, dst
        src = str(src)
        dst = str(dst)
        dst = self.ip + &#34;:&#34; + dst
        if rev:
            src, dst = dst, src

        # Get rsync version
        v = sub_run(&#34;rsync --version&#34;, shell=True).splitlines()[0].split()
        i = v.index(&#34;version&#34;)
        v = v[i+1]  # =&gt; &#39;3.2.3&#39;
        v = [int(w) for w in v.split(&#34;.&#34;)]
        has_prog2 = (v[0] &gt;= 3) and (v[1] &gt;= 1)

        # Show progress
        if prog and has_prog2:
            prog = (&#34;--info=progress2&#34;, &#34;--no-inc-recursive&#34;)
        else:
            prog = []

        # Use multiplex
        multiplex = []
        if use_M:
            multiplex = &#34;-e&#34;, self.ssh_M
        else:
            multiplex = []

        # Assemble command
        cmd = [&#34;rsync&#34;, &#34;-azh&#34;, *prog, *multiplex, *opts, src, dst]

        if dry:
            # Dry run
            return &#34; &#34;.join(cmd)
        else:
            # Sync
            subprocess.run(cmd, check=True)
            return None


def submit_job_GCP(xps_path, **kwargs):
    &#34;&#34;&#34;GCP/HTCondor launcher&#34;&#34;&#34;
    sc = SubmissionConnection(xps_path, **kwargs)
    _sync_job(sc)
    _submit_job(sc)

    # Prepare download command
    print(&#34;To download results (before completion) use:&#34;)
    xcldd = [&#34;xp.com&#34;, &#34;DAPPER&#34;, &#34;runlog&#34;, &#34;err&#34;]  # &#34;out\\.*&#34;
    xcldd = [&#34;--exclude=&#34;+x for x in xcldd]
    print(sc.rsync(
        xps_path.parent, f&#34;~/{xps_path.name}&#34;,
        rev=True, opts=xcldd, dry=True, use_M=False))

    try:
        _monitor_progress(sc)
    except (KeyboardInterrupt, Exception):
        inpt = input(&#34;Do you wish to clear the job queue? (Y/n): &#34;).lower()
        if inpt in [&#34;&#34;, &#34;y&#34;, &#34;yes&#34;]:
            print(&#34;Clearing queue&#34;)
            _clear_queue(sc)
        raise
    else:
        print(&#34;Downloading results&#34;)
        sc.rsync(xps_path.parent, f&#34;~/{xps_path.name}&#34;,
                 rev=True, opts=xcldd, prog=True)
    finally:
        # let user know smth&#39;s happenin
        # print(&#34;Checking for autoscaler cron job:&#34;)
        if not _detect_autoscaler(sc):
            print(&#34;Warning: autoscaler.py NOT detected!\n    &#34;
                  &#34;Shut down the compute nodes yourself using:\n    &#34;
                  &#34;gcloud compute instance-groups managed &#34;
                  &#34;resize condor-compute-pvm-igm --size 0&#34;)

        # Check for errors among jobs
        nHeld = _get_job_status(sc)[&#34;held&#34;]
        if nHeld:
            if nHeld == sc.nJobs:
                print(&#34;NB: All jobs failed&#34;)
            else:
                print(&#34;NB: There were %d failed jobs&#34; % nHeld)
            # Print path of error messages for a (the first found) failed job
            for d in list_job_dirs(sc.xps_path):
                if (d / &#34;xp&#34;).stat().st_size == 0:
                    print(f&#34;View error message at {d / &#39;out&#39;}&#34;)
                    break


def _detect_autoscaler(self, minutes=10):
    &#34;&#34;&#34;Grep syslog for autoscaler.

    Also get remote&#39;s date (time), to avoid another (slow) ssh.
    &#34;&#34;&#34;
    command = &#34;&#34;&#34;grep CRON /var/log/syslog | grep autoscaler | tail; date&#34;&#34;&#34;
    output  = self.remote_cmd(command).splitlines()
    recent_crons, now = output[:-1], output[-1]

    if not recent_crons:
        return False

    # Get timestamp of last cron job
    last_cron = recent_crons[-1].split(self.name)[0]
    log_time  = datetime_parse(last_cron).replace(tzinfo=timezone.utc)
    now       = datetime_parse(now)
    pause     = timedelta(minutes=minutes)

    if log_time + pause &lt; now:
        return False

    return True


def _submit_job(self):
    print(&#34;Submitting jobs&#34;)
    xps_path = self.xps_path

    # I used to have a timeout in the remote_cmd() below,
    # because I thought that ssh would hang when the condor submission
    # was taking long. However, I think it is simply the submission
    # being slow, and we had better wait for it to finish,
    # because querying condor_q before then will fail,
    # causing _monitor_progress to crash.
    if self.nJobs &gt; 4000:
        print(&#34;This might take a while&#34;)

    self.remote_cmd(
        f&#34;&#34;&#34;cd {xps_path.name}; condor_submit&#34;&#34;&#34;
        f&#34;&#34;&#34; -batch-name {xps_path.name} submit-description&#34;&#34;&#34;)


def _sync_job(self):
    print(&#34;Syncing %d jobs&#34; % self.nJobs)
    xps_path = self.xps_path

    # NB: --delete =&gt; Must precede other rsync&#39;s!
    self.rsync(xps_path, &#34;~/&#34;, &#34;--delete&#34;)

    htcondor = str(rc.dirs.DAPPER/&#34;dapper&#34;/&#34;tools&#34;/&#34;remote&#34;/&#34;htcondor&#34;) + os.sep
    self.rsync(htcondor, &#34;~/&#34;+xps_path.name)

    _sync_DAPPER(self)


def _sync_DAPPER(self):
    &#34;&#34;&#34;Sync DAPPER (as on work-tree, not a specific version) to GCP.&#34;&#34;&#34;
    # Get list of files: whatever mentioned by .git
    repo  = f&#34;--git-dir={rc.dirs.DAPPER}/.git&#34;
    files = sub_run(f&#34;git {repo} ls-tree -r --name-only HEAD&#34;, shell=True).split()

    def xcldd(f):
        return f.startswith(&#34;docs/&#34;) or f.endswith(&#34;.jpg&#34;) or f.endswith(&#34;.png&#34;)
    files = [f for f in files if not xcldd(f)]

    with tempfile.NamedTemporaryFile(&#34;w&#34;, delete=False) as synclist:
        print(&#34;\n&#34;.join(files), file=synclist)

    print(&#34;Syncing DAPPER&#34;)
    try:
        self.rsync(
            rc.dirs.DAPPER,
            f&#34;~/{self.xps_path.name}/DAPPER&#34;,
            &#34;--files-from=&#34;+synclist.name)
    except subprocess.SubprocessError as error:
        print(error.stderr)
        print(&#34;Did you mv/rm files (and not register it with `git`)?&#34;)
        raise


def print_condor_status(self):
    status = &#34;&#34;&#34;`condor_status` -total&#34;&#34;&#34;
    status = self.remote_cmd(status)
    if status:
        print(status, &#34;:&#34;)
        for line in status.splitlines()[::4]:
            print(line)
    else:
        print(&#34;[No compute nodes found]&#34;)


def _clear_queue(self):
    &#34;&#34;&#34;Use `condor_rm` to clear the job queue of the submission.&#34;&#34;&#34;
    try:
        batch = f&#34;&#34;&#34;-constraint &#39;JobBatchName == &#34;{self.xps_path.name}&#34;&#39;&#34;&#34;&#34;
        self.remote_cmd(f&#34;&#34;&#34;condor_rm {batch}&#34;&#34;&#34;)
        print(&#34;Queue cleared.&#34;)
    except subprocess.SubprocessError as error:
        if &#34;matching&#34; in error.args[0]:
            # Queue probably already cleared, as happens upon
            # KeyboardInterrupt, when there&#39;s also &#34;held&#34; jobs.
            pass
        else:
            raise


def _get_job_status(self):
    &#34;&#34;&#34;Parse `condor_q` to get number idle, held, etc, jobs&#34;&#34;&#34;
    # The autoscaler.py script from Google uses
    # &#39;condor_q -totals -format &#34;%d &#34; Jobs -format &#34;%d &#34; Idle -format &#34;%d &#34; Held&#39;
    # But in both condor versions I&#39;ve tried, -totals does not mix well with -format,
    # and the ClassAd attributes (&#34;Jobs&#34;, &#34;Idle&#34;, &#34;Held&#34;) are not available,
    # as listed by:
    #  - condor_q -l
    #  - Appendix &#34;Job ClassAd Attributes&#34; of the condor-manual (online).
    #  Condor version 8.6 (higher than 8.4 used by GCP tutorial)
    #  enables labelling jobs with -batch-name, and thus multiple jobs
    #  can be submitted and run (queried for progress, rm&#39;d, downloaded) simultaneously.
    #  One alternative is to query job status with
    #  condor_q -constraint &#39;JobStatus == 5&#39;,
    #  but I prefer to parse the -totals output instead.

    batch = f&#34;&#34;&#34;-constraint &#39;JobBatchName == &#34;{self.xps_path.name}&#34;&#39;&#34;&#34;&#34;
    qsum = self.remote_cmd(f&#34;&#34;&#34;condor_q {batch}&#34;&#34;&#34;).split()
    status = dict(jobs=&#34;jobs;&#34;, completed=&#34;completed,&#34;, removed=&#34;removed,&#34;,
                  idle=&#34;idle,&#34;, running=&#34;running,&#34;, held=&#34;held,&#34;, suspended=&#34;suspended&#34;)
    # Another way to get total num. of jobs:
    # int(self.remote_cmd(
    #     f&#34;&#34;&#34;cd {self.xps_path.name}; ls -1 | grep -o &#39;[0-9]*&#39; | wc -l&#34;&#34;&#34;))
    # Another way to parse qsum:
    # int(re.search(&#34;&#34;&#34;(\d+) idle&#34;&#34;&#34;,condor_q).group(1))
    return {k: int(qsum[qsum.index(v)-1]) for k, v in status.items()}


def _monitor_progress(self):
    &#34;&#34;&#34;Use condor_q to monitor job progress.&#34;&#34;&#34;
    num_jobs = self.nJobs
    pbar = tqdm(total=num_jobs, desc=&#34;Processing jobs&#34;)
    try:
        unfinished = num_jobs
        while unfinished:
            job_status     = _get_job_status(self)
            unlisted       = num_jobs - job_status[&#34;jobs&#34;]  # completed w/ success
            finished       = job_status[&#34;held&#34;] + unlisted  # failed + suceeded
            unfinished_new = num_jobs - finished
            increment      = unfinished - unfinished_new
            unfinished     = unfinished_new
            # print(job_status)
            pbar.update(increment)
            time.sleep(1)  # dont clog the ssh connection
    except (KeyboardInterrupt, Exception):
        print(&#34;Some kind of exception occured,&#34;
              &#34; while %d jobs have not even run.&#34; % unfinished)
        raise
    finally:
        pbar.close()


def list_job_dirs(xps_path):
    dirs = sorted_human(os.listdir(xps_path))
    dirs = [xps_path/d for d in dirs]
    dirs = [d for d in dirs if d.is_dir() and d.stem.isnumeric()]
    return dirs


def get_ip(instance):
    &#34;&#34;&#34;Get ip-address of instance.

    NB: the use of IP rather than the `Host` listed in `.ssh/config`
    (eg `condor-submit.us-central1-f.mc-tut`,
    as generated by `gcloud compute config-ssh`)
    requires `AddKeysToAgent yes` under `Host *` in `.ssh/config`,
    and that you&#39;ve already logged into the instance once using (eg)
    `ssh condor-submit.us-central1-f.mc-tut`.
    &#34;&#34;&#34;
    # cloud.google.com/compute/docs/instances/view-ip-address
    getip = &#39;get(networkInterfaces[0].accessConfigs[0].natIP)&#39;
    ip = sub_run((f&#34;gcloud compute instances describe {instance}&#34;
                 f&#34; --format={getip}&#34;).split())
    return ip.strip()

    # # Parse ssh/config for the &#34;Host&#34; of condor-submit.
    # # Q: how reliable/portable is it?
    # from pathlib import Path
    # with open(Path(&#34;~&#34;).expanduser()/&#34;.ssh&#34;/&#34;config&#34;) as ssh_config:
    #     for ln in ssh_config:
    #         if ln.startswith(&#34;Host condor-submit&#34;):
    #             break
    #     else:
    #         raise RuntimeError(
    #             &#34;Did not find condor-submit Host in .ssh/config.&#34;)
    #     return ln[ln.index(&#34;condor&#34;):].strip()


def sub_run(*args, check=True, capture_output=True, text=True, **kwargs):
    r&#34;&#34;&#34;Do `subprocess.run`, with responsive defaults.

    Examples:
    &gt;&gt;&gt; gitfiles = sub_run([&#34;git&#34;, &#34;ls-tree&#34;, &#34;-r&#34;, &#34;--name-only&#34;, &#34;HEAD&#34;])  # or:
    &gt;&gt;&gt; # gitfiles = sub_run(&#34;git ls-tree -r --name-only HEAD&#34;, shell=True)
    &#34;&#34;&#34;
    try:
        x = subprocess.run(
            *args, **kwargs,
            check=check, capture_output=capture_output, text=text)

    except subprocess.CalledProcessError as error:
        if capture_output:
            # error.args += (f&#34;The stderr is: \n\n{error.stderr}&#34;,)
            # The above won&#39;t get printed because CalledProcessError.__str__
            # is non-standard. Instead, print stdout (on top of stack trace):
            print(error.stderr)
        else:
            pass  # w/o capture_output, error is automatically printed.
        raise

    if capture_output:
        return x.stdout</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dapper.tools.remote.uplink.submit_job_GCP"><code class="name flex">
<span>def <span class="ident">submit_job_GCP</span></span>(<span>xps_path, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>GCP/HTCondor launcher</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/nansencenter/DAPPER/blob/master/dapper/tools/remote/uplink.py#L107-L153" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def submit_job_GCP(xps_path, **kwargs):
    &#34;&#34;&#34;GCP/HTCondor launcher&#34;&#34;&#34;
    sc = SubmissionConnection(xps_path, **kwargs)
    _sync_job(sc)
    _submit_job(sc)

    # Prepare download command
    print(&#34;To download results (before completion) use:&#34;)
    xcldd = [&#34;xp.com&#34;, &#34;DAPPER&#34;, &#34;runlog&#34;, &#34;err&#34;]  # &#34;out\\.*&#34;
    xcldd = [&#34;--exclude=&#34;+x for x in xcldd]
    print(sc.rsync(
        xps_path.parent, f&#34;~/{xps_path.name}&#34;,
        rev=True, opts=xcldd, dry=True, use_M=False))

    try:
        _monitor_progress(sc)
    except (KeyboardInterrupt, Exception):
        inpt = input(&#34;Do you wish to clear the job queue? (Y/n): &#34;).lower()
        if inpt in [&#34;&#34;, &#34;y&#34;, &#34;yes&#34;]:
            print(&#34;Clearing queue&#34;)
            _clear_queue(sc)
        raise
    else:
        print(&#34;Downloading results&#34;)
        sc.rsync(xps_path.parent, f&#34;~/{xps_path.name}&#34;,
                 rev=True, opts=xcldd, prog=True)
    finally:
        # let user know smth&#39;s happenin
        # print(&#34;Checking for autoscaler cron job:&#34;)
        if not _detect_autoscaler(sc):
            print(&#34;Warning: autoscaler.py NOT detected!\n    &#34;
                  &#34;Shut down the compute nodes yourself using:\n    &#34;
                  &#34;gcloud compute instance-groups managed &#34;
                  &#34;resize condor-compute-pvm-igm --size 0&#34;)

        # Check for errors among jobs
        nHeld = _get_job_status(sc)[&#34;held&#34;]
        if nHeld:
            if nHeld == sc.nJobs:
                print(&#34;NB: All jobs failed&#34;)
            else:
                print(&#34;NB: There were %d failed jobs&#34; % nHeld)
            # Print path of error messages for a (the first found) failed job
            for d in list_job_dirs(sc.xps_path):
                if (d / &#34;xp&#34;).stat().st_size == 0:
                    print(f&#34;View error message at {d / &#39;out&#39;}&#34;)
                    break</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.print_condor_status"><code class="name flex">
<span>def <span class="ident">print_condor_status</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/nansencenter/DAPPER/blob/master/dapper/tools/remote/uplink.py#L236-L244" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def print_condor_status(self):
    status = &#34;&#34;&#34;`condor_status` -total&#34;&#34;&#34;
    status = self.remote_cmd(status)
    if status:
        print(status, &#34;:&#34;)
        for line in status.splitlines()[::4]:
            print(line)
    else:
        print(&#34;[No compute nodes found]&#34;)</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.list_job_dirs"><code class="name flex">
<span>def <span class="ident">list_job_dirs</span></span>(<span>xps_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/nansencenter/DAPPER/blob/master/dapper/tools/remote/uplink.py#L314-L318" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def list_job_dirs(xps_path):
    dirs = sorted_human(os.listdir(xps_path))
    dirs = [xps_path/d for d in dirs]
    dirs = [d for d in dirs if d.is_dir() and d.stem.isnumeric()]
    return dirs</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.get_ip"><code class="name flex">
<span>def <span class="ident">get_ip</span></span>(<span>instance)</span>
</code></dt>
<dd>
<div class="desc"><p>Get ip-address of instance.</p>
<p>NB: the use of IP rather than the <code>Host</code> listed in <code>.ssh/config</code>
(eg <code>condor-submit.us-central1-f.mc-tut</code>,
as generated by <code>gcloud compute config-ssh</code>)
requires <code>AddKeysToAgent yes</code> under <code>Host *</code> in <code>.ssh/config</code>,
and that you've already logged into the instance once using (eg)
<code>ssh condor-submit.us-central1-f.mc-tut</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/nansencenter/DAPPER/blob/master/dapper/tools/remote/uplink.py#L321-L347" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def get_ip(instance):
    &#34;&#34;&#34;Get ip-address of instance.

    NB: the use of IP rather than the `Host` listed in `.ssh/config`
    (eg `condor-submit.us-central1-f.mc-tut`,
    as generated by `gcloud compute config-ssh`)
    requires `AddKeysToAgent yes` under `Host *` in `.ssh/config`,
    and that you&#39;ve already logged into the instance once using (eg)
    `ssh condor-submit.us-central1-f.mc-tut`.
    &#34;&#34;&#34;
    # cloud.google.com/compute/docs/instances/view-ip-address
    getip = &#39;get(networkInterfaces[0].accessConfigs[0].natIP)&#39;
    ip = sub_run((f&#34;gcloud compute instances describe {instance}&#34;
                 f&#34; --format={getip}&#34;).split())
    return ip.strip()

    # # Parse ssh/config for the &#34;Host&#34; of condor-submit.
    # # Q: how reliable/portable is it?
    # from pathlib import Path
    # with open(Path(&#34;~&#34;).expanduser()/&#34;.ssh&#34;/&#34;config&#34;) as ssh_config:
    #     for ln in ssh_config:
    #         if ln.startswith(&#34;Host condor-submit&#34;):
    #             break
    #     else:
    #         raise RuntimeError(
    #             &#34;Did not find condor-submit Host in .ssh/config.&#34;)
    #     return ln[ln.index(&#34;condor&#34;):].strip()</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.sub_run"><code class="name flex">
<span>def <span class="ident">sub_run</span></span>(<span>*args, check=True, capture_output=True, text=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Do <code>subprocess.run</code>, with responsive defaults.</p>
<p>Examples:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; gitfiles = sub_run([&quot;git&quot;, &quot;ls-tree&quot;, &quot;-r&quot;, &quot;--name-only&quot;, &quot;HEAD&quot;])  # or:
&gt;&gt;&gt; # gitfiles = sub_run(&quot;git ls-tree -r --name-only HEAD&quot;, shell=True)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/nansencenter/DAPPER/blob/master/dapper/tools/remote/uplink.py#L350-L373" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def sub_run(*args, check=True, capture_output=True, text=True, **kwargs):
    r&#34;&#34;&#34;Do `subprocess.run`, with responsive defaults.

    Examples:
    &gt;&gt;&gt; gitfiles = sub_run([&#34;git&#34;, &#34;ls-tree&#34;, &#34;-r&#34;, &#34;--name-only&#34;, &#34;HEAD&#34;])  # or:
    &gt;&gt;&gt; # gitfiles = sub_run(&#34;git ls-tree -r --name-only HEAD&#34;, shell=True)
    &#34;&#34;&#34;
    try:
        x = subprocess.run(
            *args, **kwargs,
            check=check, capture_output=capture_output, text=text)

    except subprocess.CalledProcessError as error:
        if capture_output:
            # error.args += (f&#34;The stderr is: \n\n{error.stderr}&#34;,)
            # The above won&#39;t get printed because CalledProcessError.__str__
            # is non-standard. Instead, print stdout (on top of stack trace):
            print(error.stderr)
        else:
            pass  # w/o capture_output, error is automatically printed.
        raise

    if capture_output:
        return x.stdout</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dapper.tools.remote.uplink.SubmissionConnection"><code class="flex name class">
<span>class <span class="ident">SubmissionConnection</span></span>
<span>(</span><span>xps_path, name='condor-submit', zone='us-central1-f', proj='mc-tut')</span>
</code></dt>
<dd>
<div class="desc"><p>Establish multiplexed ssh to a given submit-node for a given xps_path.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/nansencenter/DAPPER/blob/master/dapper/tools/remote/uplink.py#L21-L104" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class SubmissionConnection:
    &#34;&#34;&#34;Establish multiplexed ssh to a given submit-node for a given xps_path.&#34;&#34;&#34;

    def __init__(self,
                 xps_path,
                 name=&#34;condor-submit&#34;,
                 zone=&#34;us-central1-f&#34;,
                 proj=&#34;mc-tut&#34;):
        # Job info
        self.xps_path = xps_path
        self.nJobs    = len(list_job_dirs(xps_path))
        # Submit-node info
        self.name     = name
        self.proj     = proj
        self.zone     = zone
        self.host     = f&#34;{name}.{zone}.{proj}&#34;
        # instance name (as viewed by system ssh)
        self.ip       = get_ip(name)

        print(&#34;Preparing ssh connection&#34;)
        sub_run(&#34;gcloud compute config-ssh&#34;, shell=True)
        # Use multiplexing to enable simultaneous connections.
        # Possible alternative: alter MaxStartups in sshd_config,
        # or other configurations:
        # - https://stackoverflow.com/a/36654900/38281
        # - https://unix.stackexchange.com/a/226460
        # - https://superuser.com/a/1032667/142925
        self.ssh_M = (
            &#39;&#39;&#39;ssh -o ControlMaster=auto&#39;&#39;&#39;
            &#39;&#39;&#39; -o ControlPath=~/.ssh/%r@%h:%p.socket -o ControlPersist=1m&#39;&#39;&#39;)

        # print_condor_status()
        print(&#34;autoscaler.py%s detected&#34; % (&#34;&#34; if _detect_autoscaler(self) else &#34; NOT&#34;))

    def remote_cmd(self, cmd_string, **kwargs):
        &#34;&#34;&#34;Run command at self.host via multiplexed ssh.&#34;&#34;&#34;
        # Old version (uses gcloud):
        #     command = &#34;&#34;&#34;--command=&#34;&#34;&#34; + command
        #     connect = &#34;gcloud compute ssh condor-submit&#34;.split()
        #     output = sub_run(connect + [command])
        return sub_run([*self.ssh_M.split(), self.ip, cmd_string], **kwargs)

    def rsync(self, src, dst, opts=(), rev=False, prog=False, dry=False, use_M=True):
        # Prepare: opts
        if isinstance(opts, str):
            opts = opts.split()

        # Prepare: src, dst
        src = str(src)
        dst = str(dst)
        dst = self.ip + &#34;:&#34; + dst
        if rev:
            src, dst = dst, src

        # Get rsync version
        v = sub_run(&#34;rsync --version&#34;, shell=True).splitlines()[0].split()
        i = v.index(&#34;version&#34;)
        v = v[i+1]  # =&gt; &#39;3.2.3&#39;
        v = [int(w) for w in v.split(&#34;.&#34;)]
        has_prog2 = (v[0] &gt;= 3) and (v[1] &gt;= 1)

        # Show progress
        if prog and has_prog2:
            prog = (&#34;--info=progress2&#34;, &#34;--no-inc-recursive&#34;)
        else:
            prog = []

        # Use multiplex
        multiplex = []
        if use_M:
            multiplex = &#34;-e&#34;, self.ssh_M
        else:
            multiplex = []

        # Assemble command
        cmd = [&#34;rsync&#34;, &#34;-azh&#34;, *prog, *multiplex, *opts, src, dst]

        if dry:
            # Dry run
            return &#34; &#34;.join(cmd)
        else:
            # Sync
            subprocess.run(cmd, check=True)
            return None</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="dapper.tools.remote.uplink.SubmissionConnection.remote_cmd"><code class="name flex">
<span>def <span class="ident">remote_cmd</span></span>(<span>self, cmd_string, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Run command at self.host via multiplexed ssh.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/nansencenter/DAPPER/blob/master/dapper/tools/remote/uplink.py#L55-L61" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def remote_cmd(self, cmd_string, **kwargs):
    &#34;&#34;&#34;Run command at self.host via multiplexed ssh.&#34;&#34;&#34;
    # Old version (uses gcloud):
    #     command = &#34;&#34;&#34;--command=&#34;&#34;&#34; + command
    #     connect = &#34;gcloud compute ssh condor-submit&#34;.split()
    #     output = sub_run(connect + [command])
    return sub_run([*self.ssh_M.split(), self.ip, cmd_string], **kwargs)</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.SubmissionConnection.rsync"><code class="name flex">
<span>def <span class="ident">rsync</span></span>(<span>self, src, dst, opts=(), rev=False, prog=False, dry=False, use_M=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/nansencenter/DAPPER/blob/master/dapper/tools/remote/uplink.py#L63-L104" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def rsync(self, src, dst, opts=(), rev=False, prog=False, dry=False, use_M=True):
    # Prepare: opts
    if isinstance(opts, str):
        opts = opts.split()

    # Prepare: src, dst
    src = str(src)
    dst = str(dst)
    dst = self.ip + &#34;:&#34; + dst
    if rev:
        src, dst = dst, src

    # Get rsync version
    v = sub_run(&#34;rsync --version&#34;, shell=True).splitlines()[0].split()
    i = v.index(&#34;version&#34;)
    v = v[i+1]  # =&gt; &#39;3.2.3&#39;
    v = [int(w) for w in v.split(&#34;.&#34;)]
    has_prog2 = (v[0] &gt;= 3) and (v[1] &gt;= 1)

    # Show progress
    if prog and has_prog2:
        prog = (&#34;--info=progress2&#34;, &#34;--no-inc-recursive&#34;)
    else:
        prog = []

    # Use multiplex
    multiplex = []
    if use_M:
        multiplex = &#34;-e&#34;, self.ssh_M
    else:
        multiplex = []

    # Assemble command
    cmd = [&#34;rsync&#34;, &#34;-azh&#34;, *prog, *multiplex, *opts, src, dst]

    if dry:
        # Dry run
        return &#34; &#34;.join(cmd)
    else:
        # Sync
        subprocess.run(cmd, check=True)
        return None</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="DAPPER" href="https://nansencenter.github.io/DAPPER">
<img src="https://raw.githubusercontent.com/nansencenter/DAPPER/master/docs/imgs/logo_wtxt.png" alt="">
<!-- can add style="width:200px;" to img -->
</a>
</header>
<div class="gcse-search" style="height: 70px"
data-as_oq="inurl:github.com/nansencenter/DAPPER site:nansencenter.github.io/DAPPER"
data-gaCategoryParameter="dapper.tools.remote.uplink">
</div>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dapper.tools.remote" href="index.html">dapper.tools.remote</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="dapper.tools.remote.uplink.submit_job_GCP" href="#dapper.tools.remote.uplink.submit_job_GCP">submit_job_GCP</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.print_condor_status" href="#dapper.tools.remote.uplink.print_condor_status">print_condor_status</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.list_job_dirs" href="#dapper.tools.remote.uplink.list_job_dirs">list_job_dirs</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.get_ip" href="#dapper.tools.remote.uplink.get_ip">get_ip</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.sub_run" href="#dapper.tools.remote.uplink.sub_run">sub_run</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dapper.tools.remote.uplink.SubmissionConnection" href="#dapper.tools.remote.uplink.SubmissionConnection">SubmissionConnection</a></code></h4>
<ul class="">
<li><code><a title="dapper.tools.remote.uplink.SubmissionConnection.remote_cmd" href="#dapper.tools.remote.uplink.SubmissionConnection.remote_cmd">remote_cmd</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.SubmissionConnection.rsync" href="#dapper.tools.remote.uplink.SubmissionConnection.rsync">rsync</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>